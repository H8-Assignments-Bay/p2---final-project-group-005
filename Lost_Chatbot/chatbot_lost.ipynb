{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatBot Let's Get Lost!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook ini berisi dalam pembuatan model chatbot AI menggunakan JSON format file lalu mengunakan library TensorFlow dengan Keras dalam Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\TheSevenS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Library\n",
    "import json\n",
    "import string\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Model\n",
    "import tensorflow as tf\n",
    "\n",
    "# Words Pre\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json file\n",
    "\n",
    "data_file = open('intents.json').read()\n",
    "intents_json = json.loads(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list from json\n",
    "patterns = [] # Input\n",
    "tag = [] # Classes/intent\n",
    "\n",
    "for i in range(len(intents_json['intents'])):\n",
    "    for user_patterns in intents_json['intents'][i]['patterns']:\n",
    "        patterns.append(user_patterns)\n",
    "        tag.append(intents_json['intents'][i]['tag'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patterns</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hai</td>\n",
       "      <td>greet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi</td>\n",
       "      <td>greet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Halo</td>\n",
       "      <td>greet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apa Kabar</td>\n",
       "      <td>greet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Selamat Pagi</td>\n",
       "      <td>greet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       patterns    tag\n",
       "0           Hai  greet\n",
       "1            Hi  greet\n",
       "2          Halo  greet\n",
       "3     Apa Kabar  greet\n",
       "4  Selamat Pagi  greet"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe from json\n",
    "df = pd.DataFrame({\n",
    "    'patterns': patterns,\n",
    "    'tag' : tag,\n",
    "})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Fungsi yang digunakan untuk melakukan praproses\n",
    "    \"\"\"\n",
    "    # konversi ke lowercase\n",
    "    text = text.lower()\n",
    "    # menghapus tanda baca\n",
    "    tandabaca = tuple(string.punctuation)\n",
    "    text = ''.join(ch for ch in text if ch not in tandabaca)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'halo boleh bantuannya'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Konfirmasi function diatas\n",
    "kalimat = 'Halo boleh bantuannya?'\n",
    "clean_text(kalimat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             hai\n",
       "1              hi\n",
       "2            halo\n",
       "3       apa kabar\n",
       "4    selamat pagi\n",
       "Name: clean_patterns, dtype: object"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_patterns'] = df['patterns'].apply(clean_text)\n",
    "df['clean_patterns'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setelah melakukan cleaning data, selanjutnya saya akan menggunakan nltk.word_tokenize untuk menggabungkan list kalimat yang ada untuk menjadi satu kalimat berdasarkan tags.\n",
    "\n",
    "Tokenizing merupakan salah satu jenis preprocessing yang dilakukan sebelum membuat machine learning model. Tokenizing merupakan hal dasar pada data text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create corpus\n",
    "words = set([\n",
    "    word for word in df['clean_patterns'] for word in word_tokenize(word)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_size = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106 7\n"
     ]
    }
   ],
   "source": [
    "# Pengunaan tokenize pada text\n",
    "df['length'] = df['clean_patterns'].apply(word_tokenize).apply(len)\n",
    "sequence_length = int(round(df['length'].max(),0))\n",
    "print(corpus_size,sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(df['tag'])\n",
    "y_train = tf.keras.utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bye' 'canda' 'canda_2' 'creator' 'greet' 'nama' 'nanya_apps'\n",
      " 'nanya_gobot' 'pilihan' 'pilihan_2' 'pilihan_3' 'thanks' 'weblink']\n"
     ]
    }
   ],
   "source": [
    "print(encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "print(len(encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorization**\n",
    "\n",
    "Vektorisasi merupakan metode *bag of words* merupakan metode pengumpulan vocab pada corpus, metode yang saya gunakan pada hal ini adalah text vectorization yang terdapat pada TensorFlow kit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=corpus_size,\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    split='whitespace',\n",
    "    ngrams=None,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length\n",
    ")\n",
    "vect.adapt(df['clean_patterns'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7,), dtype=int64, numpy=array([86,  1,  1,  0,  0,  0,  0], dtype=int64)>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = 'halo boleh bantuannya?'\n",
    "vect(clean_text(check))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=corpus_size,\n",
    "    output_dim=16,\n",
    "    input_length=sequence_length,\n",
    "    embeddings_initializer='uniform'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7, 16), dtype=float32, numpy=\n",
       "array([[ 0.03273423, -0.01466324, -0.02194047,  0.0195001 , -0.03684908,\n",
       "        -0.03700713, -0.03961496, -0.02023194,  0.04614196, -0.02123632,\n",
       "        -0.03651143, -0.02739429, -0.04634575, -0.02686164,  0.02724976,\n",
       "         0.02910655],\n",
       "       [ 0.04243039, -0.01286985,  0.0449164 ,  0.0356538 ,  0.0358046 ,\n",
       "         0.04322822, -0.01013698,  0.0157564 ,  0.03122801,  0.03352822,\n",
       "        -0.00829347, -0.03380398,  0.02241416, -0.03102088, -0.0467157 ,\n",
       "         0.04269984],\n",
       "       [ 0.04243039, -0.01286985,  0.0449164 ,  0.0356538 ,  0.0358046 ,\n",
       "         0.04322822, -0.01013698,  0.0157564 ,  0.03122801,  0.03352822,\n",
       "        -0.00829347, -0.03380398,  0.02241416, -0.03102088, -0.0467157 ,\n",
       "         0.04269984],\n",
       "       [ 0.00447254, -0.00448421, -0.01076812,  0.03249026, -0.01133959,\n",
       "        -0.03677382, -0.0172616 , -0.02712643,  0.01529939,  0.00222324,\n",
       "         0.02653955, -0.02027609, -0.02837888, -0.04144539, -0.02792264,\n",
       "        -0.00785066],\n",
       "       [ 0.00447254, -0.00448421, -0.01076812,  0.03249026, -0.01133959,\n",
       "        -0.03677382, -0.0172616 , -0.02712643,  0.01529939,  0.00222324,\n",
       "         0.02653955, -0.02027609, -0.02837888, -0.04144539, -0.02792264,\n",
       "        -0.00785066],\n",
       "       [ 0.00447254, -0.00448421, -0.01076812,  0.03249026, -0.01133959,\n",
       "        -0.03677382, -0.0172616 , -0.02712643,  0.01529939,  0.00222324,\n",
       "         0.02653955, -0.02027609, -0.02837888, -0.04144539, -0.02792264,\n",
       "        -0.00785066],\n",
       "       [ 0.00447254, -0.00448421, -0.01076812,  0.03249026, -0.01133959,\n",
       "        -0.03677382, -0.0172616 , -0.02712643,  0.01529939,  0.00222324,\n",
       "         0.02653955, -0.02027609, -0.02837888, -0.04144539, -0.02792264,\n",
       "        -0.00785066]], dtype=float32)>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(vect(clean_text(check)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this chapter we will try train our data with LSTM (Long Short Term Memory) from tensorflow to make prediction for answer of the ChatBot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional API Model\n",
    "input = tf.keras.layers.Input(shape=(1,), dtype='string')\n",
    "hidden_1 = vect(input)\n",
    "hidden_2 = embedding (hidden_1)\n",
    "hidden_3 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128))(hidden_2)\n",
    "output = tf.keras.layers.Dense(len(encoder.classes_), activation='softmax')(hidden_3)\n",
    "model = tf.keras.Model(inputs=input, outputs=output)\n",
    "\n",
    "#Compile Model\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_2 (TextV  (None, 7)                0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 7, 16)             1696      \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirectio  (None, 256)              148480    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 13)                3341      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 153,517\n",
      "Trainable params: 153,517\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Show summary of model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.012591</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.012208</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.011949</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.011445</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.010963</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  accuracy\n",
       "95  0.012591       1.0\n",
       "96  0.012208       1.0\n",
       "97  0.011949       1.0\n",
       "98  0.011445       1.0\n",
       "99  0.010963       1.0"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = model.fit(df['clean_patterns'], y_train, epochs=100, verbose=0)\n",
    "pd.DataFrame(hist.history).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Encoder\n",
    "\n",
    "with open('encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump({'config': vect.get_config(),\n",
    "             'weights': vect.get_weights()}\n",
    "            , open(\"vect.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bot_response(text):\n",
    "    \"\"\"Take text as function input then predict using model. Return response based on highest probability using numpy argmax    \n",
    "    \"\"\"\n",
    "    text = clean_text(text)\n",
    "    pred = model.predict([text])\n",
    "    res = encoder.classes_[pred.argmax()] # Mencari index yang memiliki probabilitas tertinggi\n",
    "    i = 0\n",
    "    try:\n",
    "        if vect(text).numpy().max() > 1: # If the input is known word(s)\n",
    "            while i < len(intents_json['intents']):\n",
    "                if res == intents_json['intents'][i]['tag']:\n",
    "                    responses = intents_json['intents'][i]['responses']\n",
    "                    break\n",
    "                else:\n",
    "                    i+=1\n",
    "        else: # If only unknown word(s)\n",
    "            responses = ['Maaf kawan, aku tidak mengerti perkataan mu ...']\n",
    "    except: # If empty string or any error occured\n",
    "        responses = ['GoBot tidak mengerti :( ...']\n",
    "\n",
    "    # For debugging only\n",
    "    dict_temp = []\n",
    "    for i in range(len(pred[0])):\n",
    "        temp = {encoder.classes_[i]: pred[0][i]}\n",
    "        dict_temp.append(temp)\n",
    "    print(dict_temp)\n",
    "    print(encoder.classes_[pred.argmax()])\n",
    "\n",
    "    return print(np.random.choice(responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hai'"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tes1 = 'hai'\n",
    "clean_text(tes1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 647ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.1330127e-03, 2.7066740e-06, 2.6641314e-10, 4.9044564e-09,\n",
       "        9.9672085e-01, 8.7677887e-05, 8.3971313e-10, 1.1822385e-06,\n",
       "        1.9933213e-11, 1.6379321e-08, 5.6427102e-10, 2.0471208e-03,\n",
       "        7.3999636e-06]], dtype=float32)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([clean_text(tes1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([clean_text(tes1)]).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'greet'"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_[model.predict([clean_text(tes1)]).argmax()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GoBot merupakan online Chatbot, yang dapat membantu kawan untuk mecarikan rekomendasi tempat kawan liburan.']"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intents_json['intents'][4]['responses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "[{'bye': 0.0011330127}, {'canda': 2.706674e-06}, {'canda_2': 2.6641314e-10}, {'creator': 4.9044564e-09}, {'greet': 0.99672085}, {'nama': 8.767789e-05}, {'nanya_apps': 8.3971313e-10}, {'nanya_gobot': 1.1822385e-06}, {'pilihan': 1.9933213e-11}, {'pilihan_2': 1.6379321e-08}, {'pilihan_3': 5.64271e-10}, {'thanks': 0.0020471208}, {'weblink': 7.3999636e-06}]\n",
      "greet\n",
      "Halo selamat datang\n"
     ]
    }
   ],
   "source": [
    "bot_response('Hai!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step\n",
      "[{'bye': 0.9937512}, {'canda': 0.00024376585}, {'canda_2': 3.529947e-08}, {'creator': 2.1293776e-08}, {'greet': 0.0012050346}, {'nama': 0.00017839667}, {'nanya_apps': 8.969308e-08}, {'nanya_gobot': 1.5127756e-06}, {'pilihan': 2.4669982e-09}, {'pilihan_2': 9.506941e-07}, {'pilihan_3': 1.6138257e-07}, {'thanks': 0.0027085817}, {'weblink': 0.0019101795}]\n",
      "bye\n",
      "Sampai jumpa, terima kasih telah bertanya!\n"
     ]
    }
   ],
   "source": [
    "bot_response('Dadah!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n",
      "[{'bye': 0.0011078165}, {'canda': 2.6155403e-06}, {'canda_2': 2.3204419e-10}, {'creator': 4.3474633e-09}, {'greet': 0.99700636}, {'nama': 7.960388e-05}, {'nanya_apps': 7.4272694e-10}, {'nanya_gobot': 1.0980622e-06}, {'pilihan': 1.7230505e-11}, {'pilihan_2': 1.4881637e-08}, {'pilihan_3': 4.8229154e-10}, {'thanks': 0.0017956359}, {'weblink': 6.919822e-06}]\n",
      "greet\n",
      "Halo, ada yang bisa GoBot bantu?\n"
     ]
    }
   ],
   "source": [
    "bot_response('Halo!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6d4618e7af4546ebad8ab782c82b55d92d10efd6ebf51140178db89315c3b3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
